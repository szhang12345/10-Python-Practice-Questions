{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "MSDS_458_Siying_Zhang_Assignment_2.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": false,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/szhang12345/10-Python-Practice-Questions/blob/master/MSDS_458_Siying_Zhang_Assignment_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SRLmkqrHBQS2"
      },
      "source": [
        "## MSDS458 Research Assignment 2\n",
        "## Siying Zhang"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v2-7tctUBQS2"
      },
      "source": [
        "The CIFAR-10 dataset (Canadian Institute For Advanced Research) is a collection of images that are commonly used to train machine learning and computer vision algorithms. It is one of the most widely used datasets for machine learning research. The CIFAR-10 dataset contains 60,000 32x32 color images in 10 different classes. The 10 different classes represent airplanes, cars, birds, cats, deer, dogs, frogs, horses, ships, and trucks. There are 6,000 images of each class."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sSPcQhKUBQS2"
      },
      "source": [
        "## Import packages needed "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eHmjw7WoBQS2"
      },
      "source": [
        "# Helper libraries\n",
        "import datetime\n",
        "from packaging import version\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "import sklearn\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from collections import Counter\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# TensorFlow and tf.keras\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.preprocessing import image\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras import models, layers\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
        "from tensorflow.keras.layers import Dropout, Flatten, Input, Dense"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_cXetOQXBQS3"
      },
      "source": [
        "%matplotlib inline\n",
        "np.set_printoptions(precision=3, suppress=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yIDm78WyBQS3"
      },
      "source": [
        "### Verify TensorFlow Version and Keras Version"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z54Ct2ouBQS3"
      },
      "source": [
        "print(\"This notebook requires TensorFlow 2.0 or above\")\n",
        "print(\"TensorFlow version: \", tf.__version__)\n",
        "assert version.parse(tf.__version__).release[0] >=2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3dgj2RJnBQS6"
      },
      "source": [
        "print(\"Keras version: \", keras.__version__)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m2smn4LPBQS6"
      },
      "source": [
        "## Loading cifar10 Dataset\n",
        "\n",
        "The CIFAR-10 dataset consists of 60000 32x32 colour images in 10 classes, with 6000 images per class. There are 50000 training images and 10000 test images.<br>\n",
        "\n",
        "The dataset is divided into five training batches and one test batch, each with 10000 images. The test batch contains exactly 1000 randomly-selected images from each class. The training batches contain the remaining images in random order, but some training batches may contain more images from one class than another. Between them, the training batches contain exactly 5000 images from each class.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "thzn80jMBQS6"
      },
      "source": [
        "(train_images, train_labels),(test_images, test_labels)= tf.keras.datasets.cifar10.load_data()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L1nXyX8xBQS6"
      },
      "source": [
        "* Tuple of Numpy arrays: (x_train, y_train), (x_test, y_test).\n",
        "* x_train, x_test: uint8 arrays of color image data with shapes (num_samples, 32, 32).\n",
        "* y_train, y_test: uint8 arrays of digit labels (integers in range 0-9)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0qXkOUboBQS6"
      },
      "source": [
        "## EDA Training and Test Datasets\n",
        "\n",
        "* Imported 50000 examples for training and 10000 examples for test \n",
        "* Imported 50000 labels for training and 10000 labels for test "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "un0LuWX2BQS7"
      },
      "source": [
        "print('train_images:\\t{}'.format(train_images.shape))\n",
        "print('train_labels:\\t{}'.format(train_labels.shape))\n",
        "print('test_images:\\t\\t{}'.format(test_images.shape))\n",
        "print('test_labels:\\t\\t{}'.format(test_labels.shape))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tDL-RelMBQS7"
      },
      "source": [
        "### Review labels for training dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L_gEoattBQS7"
      },
      "source": [
        "print(\"First ten labels training dataset:\\n {}\\n\".format(train_labels[0:10]))\n",
        "print(\"This output the numeric label, need to convert to item description\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fpba3_-nBQS7"
      },
      "source": [
        "### Plot Examples"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JSoJySfUBQS7"
      },
      "source": [
        "def get_three_classes(x, y):\n",
        "    def indices_of(class_id):\n",
        "        indices, _ = np.where(y == float(class_id))\n",
        "        return indices\n",
        "\n",
        "    indices = np.concatenate([indices_of(0), indices_of(1), indices_of(2)], axis=0)\n",
        "    \n",
        "    x = x[indices]\n",
        "    y = y[indices]\n",
        "    \n",
        "    count = x.shape[0]\n",
        "    indices = np.random.choice(range(count), count, replace=False)\n",
        "    \n",
        "    x = x[indices]\n",
        "    y = y[indices]\n",
        "    \n",
        "    y = tf.keras.utils.to_categorical(y)\n",
        "    \n",
        "    return x, y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w4JdRKm5BQS7"
      },
      "source": [
        "x_preview, y_preview = get_three_classes(train_images, train_labels)\n",
        "x_preview_test, y_preview_test = get_three_classes(test_images, test_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PrUAgX78BQS8"
      },
      "source": [
        "class_names_preview = ['aeroplane', 'car', 'bird']\n",
        "\n",
        "def show_random_examples(x, y, p):\n",
        "    indices = np.random.choice(range(x.shape[0]), 10, replace=False)\n",
        "    \n",
        "    x = x[indices]\n",
        "    y = y[indices]\n",
        "    p = p[indices]\n",
        "    \n",
        "    plt.figure(figsize=(10, 5))\n",
        "    for i in range(10):\n",
        "        plt.subplot(2, 5, i + 1)\n",
        "        plt.imshow(x[i])\n",
        "        plt.xticks([])\n",
        "        plt.yticks([])\n",
        "        col = 'green' if np.argmax(y[i]) == np.argmax(p[i]) else 'red'\n",
        "        plt.xlabel(class_names_preview[np.argmax(p[i])], color=col)\n",
        "    plt.show()\n",
        "\n",
        "show_random_examples(x_preview, y_preview, y_preview)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rjdk5-jvBQS8"
      },
      "source": [
        "### Random Review of Examples "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P-JkYdP9BQS8"
      },
      "source": [
        "show_random_examples(x_preview, y_preview, y_preview)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zmOULZ_8BQS8"
      },
      "source": [
        "## Preprocessing Data for Model Development"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fWrM9M7tBQS9"
      },
      "source": [
        "The labels are an array of integers, ranging from 0 to 9. These correspond to the class of clothing the image represents:\n",
        "\n",
        "|Label  |Class_  |\n",
        "|-------|--------|\n",
        "|0|\tairplane     |\n",
        "|1|\tautomobile   |\n",
        "|2|\tbird         |\n",
        "|3|\tcat          |\n",
        "|4|\tdeer         |\n",
        "|5|\tdog          |\n",
        "|6|\tfrog         |\n",
        "|7|\thorse        |\n",
        "|8|\tship         |\n",
        "|9|\ttruck        |"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0pOir5__BQS9"
      },
      "source": [
        "class_names = ['airplane'\n",
        ",'automobile'\n",
        ",'bird'\n",
        ",'cat'\n",
        ",'deer'\n",
        ",'dog'\n",
        ",'frog' \n",
        ",'horse'\n",
        ",'ship'\n",
        ",'truck']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "83IzVivSBQS9"
      },
      "source": [
        "### Image Shape\n",
        "\n",
        "The images are 32x32 NumPy arrays, with pixel values ranging from 0 to 255.\n",
        "\n",
        "1. Each element in each example is a pixel value\n",
        "2. Pixel values range from 0 to 255\n",
        "3. 0 = black\n",
        "4. 255 = white"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3i_6n6wwLle6"
      },
      "source": [
        "### Preprocessing the Examples for DNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_oNxYn2UIYdW"
      },
      "source": [
        "train_dnn = np.reshape(train_images,(50000,3072))\n",
        "test_dnn = np.reshape(test_images,(10000,3072))\n",
        "train_dnn = train_dnn.astype('float32')\n",
        "test_dnn = test_dnn.astype('float32')\n",
        "\n",
        "# Normalization of pixel values (to [0-1] range)\n",
        "\n",
        "train_dnn /= 255\n",
        "test_dnn /= 255"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nNBnEiGXBQS9"
      },
      "source": [
        "train_dnn.shape, test_dnn.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f1Bc7VFlBQS9"
      },
      "source": [
        "### Preprocessing the Examples for CNN\n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ktEa4XpQM8QR"
      },
      "source": [
        "train_cnn = train_images.astype('float32')/255.\n",
        "test_cnn = test_images.astype('float32')/255."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kPqQ-anLOrxb"
      },
      "source": [
        "train_cnn.shape, test_cnn.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DIBw_FpHBQS9"
      },
      "source": [
        "## Validating our approaches\n",
        "\n",
        "3,000 samples of our training data to use as a validation set. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xZZtmgz9Payc"
      },
      "source": [
        "##### DNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3edKz-wUPeDx"
      },
      "source": [
        "val_dnn, train_dnn = train_dnn[:3000], train_dnn[3000:] \n",
        "val_labels_dnn, train_labels_dnn = train_labels[:3000], train_labels[3000:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ooTUDJ6PvqB"
      },
      "source": [
        "val_dnn.shape, val_labels_dnn.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HtTqum4cQIsG"
      },
      "source": [
        "train_dnn.shape, train_labels_dnn.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2K6u5lDSPrBi"
      },
      "source": [
        "##### CNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1PakHtP-IYdc"
      },
      "source": [
        "train_cnn = train_images.astype('float32')/255.\n",
        "test_cnn = test_images.astype('float32')/255."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8hPeHGT5IYdd"
      },
      "source": [
        "train_cnn.shape, test_cnn.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3O46PLZ_BQS9"
      },
      "source": [
        "val_cnn, train_cnn = train_cnn[:3000], train_cnn[3000:] \n",
        "val_labels_cnn, train_labels_cnn = train_labels[:3000], train_labels[3000:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tVeC1hPGBQS9"
      },
      "source": [
        "val_cnn.shape, val_labels_cnn.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LJUZ31c5BQS9"
      },
      "source": [
        "train_cnn.shape, train_labels_cnn.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SMrBHqFDBQS-"
      },
      "source": [
        "## Create the Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2J2S6nhjIM_b"
      },
      "source": [
        "Metrics to track performance"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ubbZ1uIrIYdi"
      },
      "source": [
        "keras.backend.clear_session()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s2QCtBRLH6hO"
      },
      "source": [
        "### DNN Model Topology"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iGe9PpBDIYdk"
      },
      "source": [
        "### Experiment 1: DNN with 1 layer (no regularization)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v0TpKpp_ITDW"
      },
      "source": [
        "model1 = models.Sequential(name=\"model1\")\n",
        "model1.add(layers.Dense(128, activation='relu', input_shape=(32 * 32 * 3,)))\n",
        "model1.add(layers.Dense(10, activation='softmax'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nk2HmFrbIzYa"
      },
      "source": [
        "keras.utils.plot_model(model1, \"CIFAR10_model_1hnode.png\", show_shapes=True) # plot a graph of the model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KdG_GloeITHV"
      },
      "source": [
        "model1.summary() # prints a summary representation of the odel"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5GXVLOBNIYdn"
      },
      "source": [
        "#### Compiling Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hRVwC4VoITND"
      },
      "source": [
        "# For use with non-categorical labels\n",
        "model1.compile(optimizer='adam',\n",
        "                loss='sparse_categorical_crossentropy',\n",
        "                metrics=['accuracy'])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T02uUpBCIYdo"
      },
      "source": [
        "#### Training Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lS3KHzP8IYdp"
      },
      "source": [
        "# Start timer\n",
        "start = datetime.datetime.now()\n",
        "history = model1.fit(train_dnn\n",
        "                    ,train_labels_dnn\n",
        "                    ,epochs=200\n",
        "                    ,batch_size=100\n",
        "                    ,validation_data=(val_dnn,val_labels_dnn)\n",
        "                    ,callbacks=[tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=3)])\n",
        "# Record the time it takes\n",
        "duration = datetime.datetime.now() - start"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T8AXyivWIYdp"
      },
      "source": [
        "#### Model Predictions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xb7uWfDvIYdq"
      },
      "source": [
        "test_loss, test_accuracy = model1.evaluate(test_dnn, test_labels)\n",
        "print('test set accuracy: ', test_accuracy)\n",
        "preds = model1.predict(test_dnn)\n",
        "print('shape of preds: ', preds.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YpN0naz1IYdr"
      },
      "source": [
        "#### Model Performance and Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_6c32m50IYdr"
      },
      "source": [
        "history_dict = history.history\n",
        "history_dict.keys()\n",
        "history_df=pd.DataFrame(history_dict)\n",
        "#history_df.tail().round(3)\n",
        "last_train_acc=history_df.tail(1).accuracy.values\n",
        "last_val_acc=history_df.tail(1).val_accuracy.values\n",
        "last_train_loss=history_df.tail(1).loss.values\n",
        "last_val_loss=history_df.tail(1).val_loss.values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "79h2Ndu7IYdr"
      },
      "source": [
        "losses = history.history['loss']\n",
        "accs = history.history['accuracy']\n",
        "val_losses = history.history['val_loss']\n",
        "val_accs = history.history['val_accuracy']\n",
        "epochs = len(losses)\n",
        "plt.figure(figsize=(16, 4))\n",
        "for i, metrics in enumerate(zip([losses, accs], [val_losses, val_accs], ['Loss', 'Accuracy'])):\n",
        "    plt.subplot(1, 2, i + 1)\n",
        "    plt.plot(range(epochs), metrics[0], label='Training {}'.format(metrics[2]))\n",
        "    plt.plot(range(epochs), metrics[1], label='Validation {}'.format(metrics[2]))\n",
        "    plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g7Syzf93HQ2K"
      },
      "source": [
        "# Initialize a dictionary to track results\n",
        "metrics = list()\n",
        "data=dict()\n",
        "\n",
        "# Metrics for evaluation\n",
        "names = ['Model Name', 'Training Accuracy', \n",
        "         'Validation Accuracy', 'Test Accuracy','Training Loss', \n",
        "         'Validation Loss', 'Test Loss'\n",
        "         'Time (s)']\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W7z0xIz6IYds"
      },
      "source": [
        "metrics = list()\n",
        "data=dict()\n",
        "data['Model Name'] = \"Model1 DNN with 1 layer (no regularization)\"\n",
        "data['train_loss'] = history_dict['loss'][-1]\n",
        "data['train_acc'] = history_dict['accuracy'][-1]\n",
        "data['val_loss'] = history_dict['val_loss'][-1]\n",
        "data['val_acc'] = history_dict['val_accuracy'][-1]\n",
        "data['Test Accuracy']=test_accuracy\n",
        "data['Test Loss']=test_loss\n",
        "data['Time (s)']=duration\n",
        "metrics.append(data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yfk2EseVkznc"
      },
      "source": [
        "metrics"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cr2n993XIYdt"
      },
      "source": [
        "### Experiment 2: DNN with 2 layers (no regularization)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FavtPELAIYdt"
      },
      "source": [
        "model2 = models.Sequential(name=\"model2\")\n",
        "model2.add(layers.Dense(128, activation='relu', input_shape=(32 * 32 * 3,)))\n",
        "model2.add(layers.Dense(256, activation='relu'))\n",
        "model2.add(layers.Dense(10, activation='softmax'))\n",
        "model2.summary() # prints a summary representation of the odel\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aSC3Jo6XU7m6"
      },
      "source": [
        "# For use with non-categorical labels\n",
        "model2.compile(optimizer='adam',\n",
        "                loss='sparse_categorical_crossentropy',\n",
        "                metrics=['accuracy'])\n",
        "\n",
        "# Start timer\n",
        "start = datetime.datetime.now()\n",
        "history = model2.fit(train_dnn\n",
        "                    ,train_labels_dnn\n",
        "                    ,epochs=200\n",
        "                    ,batch_size=100\n",
        "                    ,validation_data=(val_dnn,val_labels_dnn)\n",
        "                    ,callbacks=[tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=3)])\n",
        "# Record the time it takes\n",
        "duration = datetime.datetime.now() - start"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UpkibS5sIYdu"
      },
      "source": [
        "test_loss, test_accuracy = model2.evaluate(test_dnn, test_labels)\n",
        "print('test set accuracy: ', test_accuracy)\n",
        "preds = model2.predict(test_dnn)\n",
        "print('shape of preds: ', preds.shape)\n",
        "history_dict = history.history\n",
        "history_dict.keys()\n",
        "history_df=pd.DataFrame(history_dict)\n",
        "#history_df.tail().round(3)\n",
        "last_train_acc=history_df.tail(1).accuracy.values\n",
        "last_val_acc=history_df.tail(1).val_accuracy.values\n",
        "last_train_loss=history_df.tail(1).loss.values\n",
        "last_val_loss=history_df.tail(1).val_loss.values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wVqGzLojIYdu"
      },
      "source": [
        "losses = history.history['loss']\n",
        "accs = history.history['accuracy']\n",
        "val_losses = history.history['val_loss']\n",
        "val_accs = history.history['val_accuracy']\n",
        "epochs = len(losses)\n",
        "plt.figure(figsize=(16, 4))\n",
        "for i, metrics in enumerate(zip([losses, accs], [val_losses, val_accs], ['Loss', 'Accuracy'])):\n",
        "    plt.subplot(1, 2, i + 1)\n",
        "    plt.plot(range(epochs), metrics[0], label='Training {}'.format(metrics[2]))\n",
        "    plt.plot(range(epochs), metrics[1], label='Validation {}'.format(metrics[2]))\n",
        "    plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uW-3vTL7ZIiN"
      },
      "source": [
        "metrics2=list()\n",
        "data2=dict()\n",
        "data2['Model Name'] = \"Model2 DNN with 2 layer (no regularization)\"\n",
        "data2['train_loss'] = history_dict['loss'][-1]\n",
        "data2['train_acc'] = history_dict['accuracy'][-1]\n",
        "data2['val_loss'] = history_dict['val_loss'][-1]\n",
        "data2['val_acc'] = history_dict['val_accuracy'][-1]\n",
        "data2['Test Accuracy']=test_accuracy\n",
        "data2['Test Loss']=test_loss\n",
        "data2['Time (s)']=duration\n",
        "metrics2.append(data2)\n",
        "metrics2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AOqljBclIYdv"
      },
      "source": [
        "### Experiment 3: DNN with 3 layers (no regularization)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PluU5ibCIYdw"
      },
      "source": [
        "model3 = models.Sequential(name=\"model3\")\n",
        "model3.add(layers.Dense(128, activation='relu', input_shape=(32 * 32 * 3,)))\n",
        "model3.add(layers.Dense(256, activation='relu'))\n",
        "model3.add(layers.Dense(512, activation='relu'))\n",
        "model3.add(layers.Dense(10, activation='softmax'))\n",
        "model3.summary() # prints a summary representation of the odel\n",
        "# For use with non-categorical labels\n",
        "model3.compile(optimizer='adam',\n",
        "                loss='sparse_categorical_crossentropy',\n",
        "                metrics=['accuracy'])\n",
        "\n",
        "# Start timer\n",
        "start = datetime.datetime.now()\n",
        "history = model3.fit(train_dnn\n",
        "                    ,train_labels_dnn\n",
        "                    ,epochs=200\n",
        "                    ,batch_size=100\n",
        "                    ,validation_data=(val_dnn,val_labels_dnn)\n",
        "                    ,callbacks=[tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=3)])\n",
        "# Record the time it takes\n",
        "duration = datetime.datetime.now() - start"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NcbWTQpnIYdx"
      },
      "source": [
        "test_loss, test_accuracy = model3.evaluate(test_dnn, test_labels)\n",
        "print('test set accuracy: ', test_accuracy)\n",
        "preds = model3.predict(test_dnn)\n",
        "print('shape of preds: ', preds.shape)\n",
        "history_dict = history.history\n",
        "history_dict.keys()\n",
        "history_df=pd.DataFrame(history_dict)\n",
        "#history_df.tail().round(3)\n",
        "last_train_acc=history_df.tail(1).accuracy.values\n",
        "last_val_acc=history_df.tail(1).val_accuracy.values\n",
        "last_train_loss=history_df.tail(1).loss.values\n",
        "last_val_loss=history_df.tail(1).val_loss.values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Juw7peI6IYdx"
      },
      "source": [
        "losses = history.history['loss']\n",
        "accs = history.history['accuracy']\n",
        "val_losses = history.history['val_loss']\n",
        "val_accs = history.history['val_accuracy']\n",
        "epochs = len(losses)\n",
        "plt.figure(figsize=(16, 4))\n",
        "for i, metrics in enumerate(zip([losses, accs], [val_losses, val_accs], ['Loss', 'Accuracy'])):\n",
        "    plt.subplot(1, 2, i + 1)\n",
        "    plt.plot(range(epochs), metrics[0], label='Training {}'.format(metrics[2]))\n",
        "    plt.plot(range(epochs), metrics[1], label='Validation {}'.format(metrics[2]))\n",
        "    plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wRtVKxKzIYdy"
      },
      "source": [
        "metrics3=list()\n",
        "data3=dict()\n",
        "data3['Model Name'] = \"Model3 DNN with 3 layer (no regularization)\"\n",
        "data3['train_loss'] = history_dict['loss'][-1]\n",
        "data3['train_acc'] = history_dict['accuracy'][-1]\n",
        "data3['val_loss'] = history_dict['val_loss'][-1]\n",
        "data3['val_acc'] = history_dict['val_accuracy'][-1]\n",
        "data3['Test Accuracy']=test_accuracy\n",
        "data3['Test Loss']=test_loss\n",
        "data3['Time (s)']=duration\n",
        "metrics3.append(data3)\n",
        "metrics3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YroY8a24IYdy"
      },
      "source": [
        "### Experiment 4: DNN with 2 layers (batch normalization regularization)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AikJMo6l1MAa"
      },
      "source": [
        "from keras.layers import BatchNormalization"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l5DYZz9pIYdz"
      },
      "source": [
        "model4 = models.Sequential(name=\"model4\")\n",
        "model4.add(layers.Dense(128, activation='relu', input_shape=(32 * 32 * 3,)))\n",
        "model4.add(layers.BatchNormalization())\n",
        "model4.add(layers.Dense(256, activation='relu'))\n",
        "model4.add(layers.BatchNormalization())\n",
        "model4.add(layers.Dense(10, activation='softmax'))\n",
        "model4.summary() # prints a summary representation of the odel\n",
        "# For use with non-categorical labels\n",
        "model4.compile(optimizer='adam',\n",
        "                loss='sparse_categorical_crossentropy',\n",
        "                metrics=['accuracy'])\n",
        "\n",
        "# Start timer\n",
        "start = datetime.datetime.now()\n",
        "history = model4.fit(train_dnn\n",
        "                    ,train_labels_dnn\n",
        "                    ,epochs=200\n",
        "                    ,batch_size=100\n",
        "                    ,validation_data=(val_dnn,val_labels_dnn)\n",
        "                    ,callbacks=[tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=3)])\n",
        "# Record the time it takes\n",
        "duration = datetime.datetime.now() - start"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q0xwm4kgIYd0"
      },
      "source": [
        "test_loss, test_accuracy = model4.evaluate(test_dnn, test_labels)\n",
        "print('test set accuracy: ', test_accuracy)\n",
        "preds = model4.predict(test_dnn)\n",
        "print('shape of preds: ', preds.shape)\n",
        "history_dict = history.history\n",
        "history_dict.keys()\n",
        "history_df=pd.DataFrame(history_dict)\n",
        "#history_df.tail().round(3)\n",
        "last_train_acc=history_df.tail(1).accuracy.values\n",
        "last_val_acc=history_df.tail(1).val_accuracy.values\n",
        "last_train_loss=history_df.tail(1).loss.values\n",
        "last_val_loss=history_df.tail(1).val_loss.values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1sJ_V9ozIYd1"
      },
      "source": [
        "losses = history.history['loss']\n",
        "accs = history.history['accuracy']\n",
        "val_losses = history.history['val_loss']\n",
        "val_accs = history.history['val_accuracy']\n",
        "epochs = len(losses)\n",
        "plt.figure(figsize=(16, 4))\n",
        "for i, metrics in enumerate(zip([losses, accs], [val_losses, val_accs], ['Loss', 'Accuracy'])):\n",
        "    plt.subplot(1, 2, i + 1)\n",
        "    plt.plot(range(epochs), metrics[0], label='Training {}'.format(metrics[2]))\n",
        "    plt.plot(range(epochs), metrics[1], label='Validation {}'.format(metrics[2]))\n",
        "    plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WocxbRxfIYd2"
      },
      "source": [
        "metrics4=list()\n",
        "data4=dict()\n",
        "data4['Model Name'] = \"Model4 DNN with 2 layer (batch normalization regularization)\"\n",
        "data4['train_loss'] = history_dict['loss'][-1]\n",
        "data4['train_acc'] = history_dict['accuracy'][-1]\n",
        "data4['val_loss'] = history_dict['val_loss'][-1]\n",
        "data4['val_acc'] = history_dict['val_accuracy'][-1]\n",
        "data4['Test Accuracy']=test_accuracy\n",
        "data4['Test Loss']=test_loss\n",
        "data4['Time (s)']=duration\n",
        "metrics3.append(data4)\n",
        "metrics3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tW9pRNhiRQeM"
      },
      "source": [
        "metrics3.append(data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0emdbyoART3E"
      },
      "source": [
        "metrics3.append(data2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UoAtuOJMIYd3"
      },
      "source": [
        "### Experiment 5: DNN with 3 layers (batch normalization regularization)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OSuZwMVlIYd3"
      },
      "source": [
        "model5 = models.Sequential(name=\"model5\")\n",
        "model5.add(layers.Dense(128, activation='relu', input_shape=(32 * 32 * 3,)))\n",
        "model5.add(layers.BatchNormalization())\n",
        "model5.add(layers.Dense(256, activation='relu'))\n",
        "model5.add(layers.BatchNormalization())\n",
        "model5.add(layers.Dense(512, activation='relu'))\n",
        "model5.add(layers.BatchNormalization())\n",
        "model5.add(layers.Dense(10, activation='softmax'))\n",
        "model5.summary() # prints a summary representation of the odel\n",
        "model5.compile(optimizer='adam',\n",
        "                loss='sparse_categorical_crossentropy',\n",
        "                metrics=['accuracy'])\n",
        "\n",
        "# Start timer\n",
        "start = datetime.datetime.now()\n",
        "history = model5.fit(train_dnn\n",
        "                    ,train_labels_dnn\n",
        "                    ,epochs=200\n",
        "                    ,batch_size=100\n",
        "                    ,validation_data=(val_dnn,val_labels_dnn)\n",
        "                    ,callbacks=[tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=3)])\n",
        "# Record the time it takes\n",
        "duration = datetime.datetime.now() - start"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8gaat9oZIYd4"
      },
      "source": [
        "test_loss, test_accuracy = model5.evaluate(test_dnn, test_labels)\n",
        "print('test set accuracy: ', test_accuracy)\n",
        "preds = model5.predict(test_dnn)\n",
        "print('shape of preds: ', preds.shape)\n",
        "history_dict = history.history\n",
        "history_dict.keys()\n",
        "history_df=pd.DataFrame(history_dict)\n",
        "#history_df.tail().round(3)\n",
        "last_train_acc=history_df.tail(1).accuracy.values\n",
        "last_val_acc=history_df.tail(1).val_accuracy.values\n",
        "last_train_loss=history_df.tail(1).loss.values\n",
        "last_val_loss=history_df.tail(1).val_loss.values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SLY8u-oiIYd5"
      },
      "source": [
        "losses = history.history['loss']\n",
        "accs = history.history['accuracy']\n",
        "val_losses = history.history['val_loss']\n",
        "val_accs = history.history['val_accuracy']\n",
        "epochs = len(losses)\n",
        "plt.figure(figsize=(16, 4))\n",
        "for i, metrics in enumerate(zip([losses, accs], [val_losses, val_accs], ['Loss', 'Accuracy'])):\n",
        "    plt.subplot(1, 2, i + 1)\n",
        "    plt.plot(range(epochs), metrics[0], label='Training {}'.format(metrics[2]))\n",
        "    plt.plot(range(epochs), metrics[1], label='Validation {}'.format(metrics[2]))\n",
        "    plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_VyoQ1K9IYd5"
      },
      "source": [
        "metrics5=list()\n",
        "data5=dict()\n",
        "data5['Model Name'] = \"Model5 DNN with 3 layer (batch normalization regularization)\"\n",
        "data5['train_loss'] = history_dict['loss'][-1]\n",
        "data5['train_acc'] = history_dict['accuracy'][-1]\n",
        "data5['val_loss'] = history_dict['val_loss'][-1]\n",
        "data5['val_acc'] = history_dict['val_accuracy'][-1]\n",
        "data5['Test Accuracy']=test_accuracy\n",
        "data5['Test Loss']=test_loss\n",
        "data5['Time (s)']=duration\n",
        "metrics3.append(data5)\n",
        "metrics3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VPMFbaFl5si1"
      },
      "source": [
        "metrics3=sorted(metrics3, key = lambda i: (i['Model Name']))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZX6PtPmVIYd6"
      },
      "source": [
        "### Experiment 6: DNN with 2 layers (dropout regularization-dropout size: 0.2)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-lLj2q0I7Rup"
      },
      "source": [
        "from keras.layers import Dropout"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ejztIzrVIYd6"
      },
      "source": [
        "model6 = models.Sequential(name=\"model6\")\n",
        "model6.add(layers.Dense(128, activation='relu', input_shape=(32 * 32 * 3,)))\n",
        "model6.add(Dropout(0.2))\n",
        "model6.add(layers.Dense(256, activation='relu'))\n",
        "model6.add(Dropout(0.2))\n",
        "model6.add(layers.Dense(10, activation='softmax'))\n",
        "model6.summary() # prints a summary representation of the odel\n",
        "# For use with non-categorical labels\n",
        "model6.compile(optimizer='adam',\n",
        "                loss='sparse_categorical_crossentropy',\n",
        "                metrics=['accuracy'])\n",
        "\n",
        "# Start timer\n",
        "start = datetime.datetime.now()\n",
        "history = model6.fit(train_dnn\n",
        "                    ,train_labels_dnn\n",
        "                    ,epochs=200\n",
        "                    ,batch_size=100\n",
        "                    ,validation_data=(val_dnn,val_labels_dnn)\n",
        "                    ,callbacks=[tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=3)])\n",
        "# Record the time it takes\n",
        "duration = datetime.datetime.now() - start"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VQzNjB9wIYd7"
      },
      "source": [
        "test_loss, test_accuracy = model6.evaluate(test_dnn, test_labels)\n",
        "print('test set accuracy: ', test_accuracy)\n",
        "preds = model6.predict(test_dnn)\n",
        "print('shape of preds: ', preds.shape)\n",
        "history_dict = history.history\n",
        "history_dict.keys()\n",
        "history_df=pd.DataFrame(history_dict)\n",
        "#history_df.tail().round(3)\n",
        "last_train_acc=history_df.tail(1).accuracy.values\n",
        "last_val_acc=history_df.tail(1).val_accuracy.values\n",
        "last_train_loss=history_df.tail(1).loss.values\n",
        "last_val_loss=history_df.tail(1).val_loss.values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DggsFUCOIYd7"
      },
      "source": [
        "losses = history.history['loss']\n",
        "accs = history.history['accuracy']\n",
        "val_losses = history.history['val_loss']\n",
        "val_accs = history.history['val_accuracy']\n",
        "epochs = len(losses)\n",
        "plt.figure(figsize=(16, 4))\n",
        "for i, metrics in enumerate(zip([losses, accs], [val_losses, val_accs], ['Loss', 'Accuracy'])):\n",
        "    plt.subplot(1, 2, i + 1)\n",
        "    plt.plot(range(epochs), metrics[0], label='Training {}'.format(metrics[2]))\n",
        "    plt.plot(range(epochs), metrics[1], label='Validation {}'.format(metrics[2]))\n",
        "    plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B_mgMAkNIYd7"
      },
      "source": [
        "data6=dict()\n",
        "data6['Model Name'] = \"Model6 DNN with 2 layers (dropout regularization-dropout size: 0.2)\"\n",
        "data6['train_loss'] = history_dict['loss'][-1]\n",
        "data6['train_acc'] = history_dict['accuracy'][-1]\n",
        "data6['val_loss'] = history_dict['val_loss'][-1]\n",
        "data6['val_acc'] = history_dict['val_accuracy'][-1]\n",
        "data6['Test Accuracy']=test_accuracy\n",
        "data6['Test Loss']=test_loss\n",
        "data6['Time (s)']=duration\n",
        "metrics3.append(data6)\n",
        "metrics3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ps-8UuupIYd8"
      },
      "source": [
        "### Experiment 7: DNN with 3 layers (dropout regularization-0.2)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sAyQhlKHIYd8"
      },
      "source": [
        "model7 = models.Sequential(name=\"model7\")\n",
        "model7.add(layers.Dense(128, activation='relu', input_shape=(32 * 32 * 3,)))\n",
        "model7.add(Dropout(0.2))\n",
        "model7.add(layers.Dense(256, activation='relu'))\n",
        "model7.add(Dropout(0.2))\n",
        "model7.add(layers.Dense(512, activation='relu'))\n",
        "model7.add(Dropout(0.2))\n",
        "model7.add(layers.Dense(10, activation='softmax'))\n",
        "model7.summary() # prints a summary representation of the odel\n",
        "# For use with non-categorical labels\n",
        "model7.compile(optimizer='adam',\n",
        "                loss='sparse_categorical_crossentropy',\n",
        "                metrics=['accuracy'])\n",
        "\n",
        "# Start timer\n",
        "start = datetime.datetime.now()\n",
        "history = model7.fit(train_dnn\n",
        "                    ,train_labels_dnn\n",
        "                    ,epochs=200\n",
        "                    ,batch_size=100\n",
        "                    ,validation_data=(val_dnn,val_labels_dnn)\n",
        "                    ,callbacks=[tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=3)])\n",
        "# Record the time it takes\n",
        "duration = datetime.datetime.now() - start"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lyIXE_3QIYd9"
      },
      "source": [
        "test_loss, test_accuracy = model7.evaluate(test_dnn, test_labels)\n",
        "print('test set accuracy: ', test_accuracy)\n",
        "preds = model7.predict(test_dnn)\n",
        "print('shape of preds: ', preds.shape)\n",
        "history_dict = history.history\n",
        "history_dict.keys()\n",
        "history_df=pd.DataFrame(history_dict)\n",
        "#history_df.tail().round(3)\n",
        "last_train_acc=history_df.tail(1).accuracy.values\n",
        "last_val_acc=history_df.tail(1).val_accuracy.values\n",
        "last_train_loss=history_df.tail(1).loss.values\n",
        "last_val_loss=history_df.tail(1).val_loss.values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BS9-jmuAIYd9"
      },
      "source": [
        "losses = history.history['loss']\n",
        "accs = history.history['accuracy']\n",
        "val_losses = history.history['val_loss']\n",
        "val_accs = history.history['val_accuracy']\n",
        "epochs = len(losses)\n",
        "plt.figure(figsize=(16, 4))\n",
        "for i, metrics in enumerate(zip([losses, accs], [val_losses, val_accs], ['Loss', 'Accuracy'])):\n",
        "    plt.subplot(1, 2, i + 1)\n",
        "    plt.plot(range(epochs), metrics[0], label='Training {}'.format(metrics[2]))\n",
        "    plt.plot(range(epochs), metrics[1], label='Validation {}'.format(metrics[2]))\n",
        "    plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4706kBFtIYd9"
      },
      "source": [
        "data7=dict()\n",
        "data7['Model Name'] = \"Model7 DNN with 3 layers (dropout regularization-0.2)\"\n",
        "data7['train_loss'] = history_dict['loss'][-1]\n",
        "data7['train_acc'] = history_dict['accuracy'][-1]\n",
        "data7['val_loss'] = history_dict['val_loss'][-1]\n",
        "data7['val_acc'] = history_dict['val_accuracy'][-1]\n",
        "data7['Test Accuracy']=test_accuracy\n",
        "data7['Test Loss']=test_loss\n",
        "data7['Time (s)']=duration\n",
        "metrics3.append(data7)\n",
        "metrics3\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZQksDxFtBQS-"
      },
      "source": [
        "### CNN Model Topology"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kUAXSso3BQS-"
      },
      "source": [
        "We use a Sequential class defined in Keras to create our model. The Conv2D and MaxPooling layers handle feature learning.  The last 3 layers, handle classification.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uNsQ6Qb-BQS-"
      },
      "source": [
        "### Compiling the model\n",
        "\n",
        "In addition to setting up our model architecture, we also need to define which algorithm should the model use in order to optimize the weights and biases as per the given data. We will use stochastic gradient descent.\n",
        "\n",
        "We also need to define a loss function. Think of this function as the difference between the predicted outputs and the actual outputs given in the dataset. This loss needs to be minimised in order to have a higher model accuracy. That's what the optimization algorithm essentially does - it minimises the loss during model training. For our multi-class classification problem, categorical cross entropy is commonly used.\n",
        "\n",
        "Finally, we will use the accuracy during training as a metric to keep track of as the model trains."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W9-z0twRIYd_"
      },
      "source": [
        "### Experiment 8: CNN with 2 convolution/max pooling layers (no regularization)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QlqxP843IYd_"
      },
      "source": [
        "model8 = models.Sequential(name=\"model8\")\n",
        "model8.add(layers.Conv2D(filters=128, kernel_size=(3, 3), strides=(1, 1), activation=tf.nn.relu,input_shape=(32, 32, 3)))\n",
        "model8.add(layers.MaxPool2D((2, 2),strides=2))\n",
        "model8.add(layers.Conv2D(filters=256, kernel_size=(3, 3), strides=(1, 1), activation=tf.nn.relu))\n",
        "model8.add(layers.MaxPool2D(pool_size=(2, 2),strides=2))\n",
        "model8.add(layers.Flatten())\n",
        "model8.add(layers.Dense(units=384, activation=tf.nn.relu))\n",
        "model8.add(layers.Dense(units=10, activation=tf.nn.softmax))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zj4d6cdYIYd_"
      },
      "source": [
        "model8.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gA3fUFQJIYeA"
      },
      "source": [
        "model8.compile(optimizer='adam',\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pi0y29ogIYeA"
      },
      "source": [
        "# Start timer\n",
        "start = datetime.datetime.now()\n",
        "history = model8.fit(train_cnn\n",
        "                    ,train_labels_cnn\n",
        "                    ,epochs=200\n",
        "                    ,batch_size=100\n",
        "                    ,validation_data=(val_cnn,val_labels_cnn)\n",
        "                    ,callbacks=[\n",
        "                    tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=3),\n",
        "                    tf.keras.callbacks.ModelCheckpoint('./models/model_{val_accuracy:.4f}.h5', save_best_only=True,\n",
        "                                        save_weights_only=False, monitor='val_accuracy')]                                                                                                           \n",
        "                   )\n",
        "#acc_train = accuracy.eval(feed_dict={X: train_images_norm, y: train_labels})\n",
        "#acc_val = accuracy.eval(feed_dict={X: val_images_norm, y: val_labels})\n",
        "#loss_train = loss.eval(feed_dict={X: train_images_norm, y: train_labels})\n",
        "#loss_val = loss.eval(feed_dict={X: val_images_norm, y: val_labels})\n",
        "#loss, accuracy = model1.evaluate(test_images_norm, test_labels)\n",
        "#print('test set accuracy: ', accuracy * 100)\n",
        "#preds = model1.predict(test_images_norm)\n",
        "#print('shape of preds: ', preds.shape)\n",
        "# Record the time it takes\n",
        "duration = datetime.datetime.now() - start\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_7-GEx-BIYeA"
      },
      "source": [
        "test_loss, test_accuracy = model8.evaluate(test_cnn, test_labels)\n",
        "print('test set accuracy: ', test_accuracy)\n",
        "preds = model8.predict(test_cnn)\n",
        "print('shape of preds: ', preds.shape)\n",
        "history_dict = history.history\n",
        "history_dict.keys()\n",
        "history_df=pd.DataFrame(history_dict)\n",
        "#history_df.tail().round(3)\n",
        "last_train_acc=history_df.tail(1).accuracy.values\n",
        "last_val_acc=history_df.tail(1).val_accuracy.values\n",
        "last_train_loss=history_df.tail(1).loss.values\n",
        "last_val_loss=history_df.tail(1).val_loss.values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-hVPRdQbIYeB"
      },
      "source": [
        "losses = history.history['loss']\n",
        "accs = history.history['accuracy']\n",
        "val_losses = history.history['val_loss']\n",
        "val_accs = history.history['val_accuracy']\n",
        "epochs = len(losses)\n",
        "plt.figure(figsize=(16, 4))\n",
        "for i, metrics in enumerate(zip([losses, accs], [val_losses, val_accs], ['Loss', 'Accuracy'])):\n",
        "    plt.subplot(1, 2, i + 1)\n",
        "    plt.plot(range(epochs), metrics[0], label='Training {}'.format(metrics[2]))\n",
        "    plt.plot(range(epochs), metrics[1], label='Validation {}'.format(metrics[2]))\n",
        "    plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_iZDXaVOVafo"
      },
      "source": [
        "metrics3=list(filter(lambda i: i['Model Name'] != 'Model8 CNN with 2 convolution/max pooling layers (no regularization)', metrics3))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GGTIBvTsIYeB"
      },
      "source": [
        "data8=dict()\n",
        "data8['Model Name'] = \"Model8 CNN with 2 convolution/max pooling layers (no regularization)\"\n",
        "data8['train_loss'] = history_dict['loss'][-1]\n",
        "data8['train_acc'] = history_dict['accuracy'][-1]\n",
        "data8['val_loss'] = history_dict['val_loss'][-1]\n",
        "data8['val_acc'] = history_dict['val_accuracy'][-1]\n",
        "data8['Test Accuracy']=test_accuracy\n",
        "data8['Test Loss']=test_loss\n",
        "data8['Time (s)']=duration\n",
        "metrics3.append(data8)\n",
        "metrics3\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kpoVIYoyIYeC"
      },
      "source": [
        "### Experiment 9: CNN with 3 convolution/max pooling layers (no regularization)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tCTEiKFzIYeC"
      },
      "source": [
        "model9 = models.Sequential(name=\"model9\")\n",
        "model9.add(layers.Conv2D(filters=128, kernel_size=(3, 3), strides=(1, 1), activation=tf.nn.relu,input_shape=(32, 32, 3)))\n",
        "model9.add(layers.MaxPool2D((2, 2),strides=2))\n",
        "model9.add(layers.Conv2D(filters=256, kernel_size=(3, 3), strides=(1, 1), activation=tf.nn.relu))\n",
        "model9.add(layers.MaxPool2D(pool_size=(2, 2),strides=2))\n",
        "model9.add(layers.Conv2D(filters=512, kernel_size=(3, 3), strides=(1, 1), activation=tf.nn.relu))\n",
        "model9.add(layers.MaxPool2D(pool_size=(2, 2),strides=2))\n",
        "model9.add(layers.Flatten())\n",
        "model9.add(layers.Dense(units=384, activation=tf.nn.relu))\n",
        "model9.add(layers.Dense(units=10, activation=tf.nn.softmax))\n",
        "model9.summary()\n",
        "model9.compile(optimizer='adam',\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
        "              metrics=['accuracy'])\n",
        "# Start timer\n",
        "start = datetime.datetime.now()\n",
        "history = model9.fit(train_cnn\n",
        "                    ,train_labels_cnn\n",
        "                    ,epochs=200\n",
        "                    ,batch_size=100\n",
        "                    ,validation_data=(val_cnn,val_labels_cnn)\n",
        "                    ,callbacks=[\n",
        "                    tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=3),\n",
        "                    tf.keras.callbacks.ModelCheckpoint('./models/model_{val_accuracy:.4f}.h5', save_best_only=True,\n",
        "                                        save_weights_only=False, monitor='val_accuracy')]                                                                                                           \n",
        "                   )\n",
        "\n",
        "# Record the time it takes\n",
        "duration = datetime.datetime.now() - start\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5PMUr2wLIYeC"
      },
      "source": [
        "test_loss, test_accuracy = model9.evaluate(test_cnn, test_labels)\n",
        "print('test set accuracy: ', test_accuracy)\n",
        "preds = model9.predict(test_cnn)\n",
        "print('shape of preds: ', preds.shape)\n",
        "history_dict = history.history\n",
        "history_dict.keys()\n",
        "history_df=pd.DataFrame(history_dict)\n",
        "#history_df.tail().round(3)\n",
        "last_train_acc=history_df.tail(1).accuracy.values\n",
        "last_val_acc=history_df.tail(1).val_accuracy.values\n",
        "last_train_loss=history_df.tail(1).loss.values\n",
        "last_val_loss=history_df.tail(1).val_loss.values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ec_hGbIYIYeD"
      },
      "source": [
        "losses = history.history['loss']\n",
        "accs = history.history['accuracy']\n",
        "val_losses = history.history['val_loss']\n",
        "val_accs = history.history['val_accuracy']\n",
        "epochs = len(losses)\n",
        "plt.figure(figsize=(16, 4))\n",
        "for i, metrics in enumerate(zip([losses, accs], [val_losses, val_accs], ['Loss', 'Accuracy'])):\n",
        "    plt.subplot(1, 2, i + 1)\n",
        "    plt.plot(range(epochs), metrics[0], label='Training {}'.format(metrics[2]))\n",
        "    plt.plot(range(epochs), metrics[1], label='Validation {}'.format(metrics[2]))\n",
        "    plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cBqy8l_EIYeD"
      },
      "source": [
        "data9=dict()\n",
        "data9['Model Name'] = \"Model9 CNN with 3 convolution/max pooling layers (no regularization)\"\n",
        "data9['train_loss'] = history_dict['loss'][-1]\n",
        "data9['train_acc'] = history_dict['accuracy'][-1]\n",
        "data9['val_loss'] = history_dict['val_loss'][-1]\n",
        "data9['val_acc'] = history_dict['val_accuracy'][-1]\n",
        "data9['Test Accuracy']=test_accuracy\n",
        "data9['Test Loss']=test_loss\n",
        "data9['Time (s)']=duration\n",
        "metrics3.append(data9)\n",
        "metrics3\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Y_BBbdYIYeE"
      },
      "source": [
        "### Experiment 10: CNN with 2 convolution/max pooling layers (batch normalization regularization)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lkzy_8DrIYeE"
      },
      "source": [
        "model10 = models.Sequential(name=\"model10\")\n",
        "model10.add(layers.Conv2D(filters=128, kernel_size=(3, 3), strides=(1, 1), activation=tf.nn.relu,input_shape=(32, 32, 3)))\n",
        "model10.add(BatchNormalization())\n",
        "model10.add(layers.MaxPool2D((2, 2),strides=2))\n",
        "model10.add(layers.Conv2D(filters=256, kernel_size=(3, 3), strides=(1, 1), activation=tf.nn.relu))\n",
        "model10.add(BatchNormalization())\n",
        "model10.add(layers.MaxPool2D(pool_size=(2, 2),strides=2))\n",
        "#model10.add(layers.Conv2D(filters=512, kernel_size=(3, 3), strides=(1, 1), activation=tf.nn.relu))\n",
        "#model10.add(BatchNormalization())\n",
        "#model10.add(layers.MaxPool2D(pool_size=(2, 2),strides=2))\n",
        "model10.add(layers.Flatten())\n",
        "model10.add(layers.Dense(units=384, activation=tf.nn.relu))\n",
        "model10.add(layers.Dense(units=10, activation=tf.nn.softmax))\n",
        "model10.summary()\n",
        "model10.compile(optimizer='adam',\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
        "              metrics=['accuracy'])\n",
        "# Start timer\n",
        "start = datetime.datetime.now()\n",
        "history = model10.fit(train_cnn\n",
        "                    ,train_labels_cnn\n",
        "                    ,epochs=200\n",
        "                    ,batch_size=100\n",
        "                    ,validation_data=(val_cnn,val_labels_cnn)\n",
        "                    ,callbacks=[\n",
        "                    tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=3)\n",
        "                    ]                                                                                                           \n",
        "                   )\n",
        "\n",
        "# Record the time it takes\n",
        "duration = datetime.datetime.now() - start\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Motquj4IYeE"
      },
      "source": [
        "test_loss, test_accuracy = model10.evaluate(test_cnn, test_labels)\n",
        "print('test set accuracy: ', test_accuracy)\n",
        "preds = model10.predict(test_cnn)\n",
        "print('shape of preds: ', preds.shape)\n",
        "history_dict = history.history\n",
        "history_dict.keys()\n",
        "history_df=pd.DataFrame(history_dict)\n",
        "#history_df.tail().round(3)\n",
        "last_train_acc=history_df.tail(1).accuracy.values\n",
        "last_val_acc=history_df.tail(1).val_accuracy.values\n",
        "last_train_loss=history_df.tail(1).loss.values\n",
        "last_val_loss=history_df.tail(1).val_loss.values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K4106dDJIYeF"
      },
      "source": [
        "losses = history.history['loss']\n",
        "accs = history.history['accuracy']\n",
        "val_losses = history.history['val_loss']\n",
        "val_accs = history.history['val_accuracy']\n",
        "epochs = len(losses)\n",
        "plt.figure(figsize=(16, 4))\n",
        "for i, metrics in enumerate(zip([losses, accs], [val_losses, val_accs], ['Loss', 'Accuracy'])):\n",
        "    plt.subplot(1, 2, i + 1)\n",
        "    plt.plot(range(epochs), metrics[0], label='Training {}'.format(metrics[2]))\n",
        "    plt.plot(range(epochs), metrics[1], label='Validation {}'.format(metrics[2]))\n",
        "    plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YCQNFAtRIYeF"
      },
      "source": [
        "data10=dict()\n",
        "data10['Model Name'] = \"Model10 CNN with 2 convolution/max pooling layers (batch normalization regularization)\"\n",
        "data10['train_loss'] = history_dict['loss'][-1]\n",
        "data10['train_acc'] = history_dict['accuracy'][-1]\n",
        "data10['val_loss'] = history_dict['val_loss'][-1]\n",
        "data10['val_acc'] = history_dict['val_accuracy'][-1]\n",
        "data10['Test Accuracy']=test_accuracy\n",
        "data10['Test Loss']=test_loss\n",
        "data10['Time (s)']=duration\n",
        "metrics3.append(data10)\n",
        "metrics3\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yoR0gVeRIYeG"
      },
      "source": [
        "### Experiment 11: CNN with 3 convolution/max pooling layers (batch normalization regularization)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ny3Sv9lGIYeG"
      },
      "source": [
        "model11 = models.Sequential(name=\"model11\")\n",
        "model11.add(layers.Conv2D(filters=128, kernel_size=(3, 3), strides=(1, 1), activation=tf.nn.relu,input_shape=(32, 32, 3)))\n",
        "model11.add(BatchNormalization())\n",
        "model11.add(layers.MaxPool2D((2, 2),strides=2))\n",
        "model11.add(layers.Conv2D(filters=256, kernel_size=(3, 3), strides=(1, 1), activation=tf.nn.relu))\n",
        "model11.add(BatchNormalization())\n",
        "model11.add(layers.MaxPool2D(pool_size=(2, 2),strides=2))\n",
        "model11.add(layers.Conv2D(filters=512, kernel_size=(3, 3), strides=(1, 1), activation=tf.nn.relu))\n",
        "model11.add(BatchNormalization())\n",
        "model11.add(layers.MaxPool2D(pool_size=(2, 2),strides=2))\n",
        "model11.add(layers.Flatten())\n",
        "model11.add(layers.Dense(units=384, activation=tf.nn.relu))\n",
        "model11.add(layers.Dense(units=10, activation=tf.nn.softmax))\n",
        "model11.summary()\n",
        "model11.compile(optimizer='adam',\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
        "              metrics=['accuracy'])\n",
        "# Start timer\n",
        "start = datetime.datetime.now()\n",
        "history = model11.fit(train_cnn\n",
        "                    ,train_labels_cnn\n",
        "                    ,epochs=200\n",
        "                    ,batch_size=100\n",
        "                    ,validation_data=(val_cnn,val_labels_cnn)\n",
        "                    ,callbacks=[\n",
        "                    tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=3)\n",
        "                    ]                                                                                                           \n",
        "                   )\n",
        "\n",
        "# Record the time it takes\n",
        "duration = datetime.datetime.now() - start\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7FQqYlKFIYeH"
      },
      "source": [
        "test_loss, test_accuracy = model11.evaluate(test_cnn, test_labels)\n",
        "print('test set accuracy: ', test_accuracy)\n",
        "preds = model11.predict(test_cnn)\n",
        "print('shape of preds: ', preds.shape)\n",
        "history_dict = history.history\n",
        "history_dict.keys()\n",
        "history_df=pd.DataFrame(history_dict)\n",
        "#history_df.tail().round(3)\n",
        "last_train_acc=history_df.tail(1).accuracy.values\n",
        "last_val_acc=history_df.tail(1).val_accuracy.values\n",
        "last_train_loss=history_df.tail(1).loss.values\n",
        "last_val_loss=history_df.tail(1).val_loss.values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cn97XwnBFfVf"
      },
      "source": [
        "##### Plotting Performance Metrics \n",
        "\n",
        "We use Matplotlib to create 2 plots--displaying the training and validation loss (resp. accuracy) for each (training) epoch side by side."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gJ7V_-kiIYeH"
      },
      "source": [
        "losses = history.history['loss']\n",
        "accs = history.history['accuracy']\n",
        "val_losses = history.history['val_loss']\n",
        "val_accs = history.history['val_accuracy']\n",
        "epochs = len(losses)\n",
        "plt.figure(figsize=(16, 4))\n",
        "for i, metrics in enumerate(zip([losses, accs], [val_losses, val_accs], ['Loss', 'Accuracy'])):\n",
        "    plt.subplot(1, 2, i + 1)\n",
        "    plt.plot(range(epochs), metrics[0], label='Training {}'.format(metrics[2]))\n",
        "    plt.plot(range(epochs), metrics[1], label='Validation {}'.format(metrics[2]))\n",
        "    plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3BV34-OkIYeI"
      },
      "source": [
        "data11=dict()\n",
        "data11['Model Name'] = \"Model11 CNN with 3 convolution/max pooling layers (batch normalization regularization)\"\n",
        "data11['train_loss'] = history_dict['loss'][-1]\n",
        "data11['train_acc'] = history_dict['accuracy'][-1]\n",
        "data11['val_loss'] = history_dict['val_loss'][-1]\n",
        "data11['val_acc'] = history_dict['val_accuracy'][-1]\n",
        "data11['Test Accuracy']=test_accuracy\n",
        "data11['Test Loss']=test_loss\n",
        "data11['Time (s)']=duration\n",
        "metrics3.append(data11)\n",
        "metrics3\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sb4jRGt6IYeJ"
      },
      "source": [
        "### Experiment 12: CNN with 2 convolution/max pooling layers (dropout regularization- dropout size: 0.2)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ppWvVNsZIYeJ"
      },
      "source": [
        "model12 = models.Sequential(name=\"model12\")\n",
        "model12.add(layers.Conv2D(filters=128, kernel_size=(3, 3), strides=(1, 1), activation=tf.nn.relu,input_shape=(32, 32, 3)))\n",
        "model12.add(layers.MaxPool2D((2, 2),strides=2))\n",
        "model12.add(layers.Dropout(0.2))\n",
        "model12.add(layers.Conv2D(filters=256, kernel_size=(3, 3), strides=(1, 1), activation=tf.nn.relu))\n",
        "model12.add(layers.MaxPool2D(pool_size=(2, 2),strides=2))\n",
        "model12.add(layers.Dropout(0.2))\n",
        "#model10.add(layers.Conv2D(filters=512, kernel_size=(3, 3), strides=(1, 1), activation=tf.nn.relu))\n",
        "#model10.add(BatchNormalization())\n",
        "#model10.add(layers.MaxPool2D(pool_size=(2, 2),strides=2))\n",
        "model12.add(layers.Flatten())\n",
        "model12.add(layers.Dense(units=384, activation=tf.nn.relu))\n",
        "model12.add(layers.Dense(units=10, activation=tf.nn.softmax))\n",
        "model12.summary()\n",
        "model12.compile(optimizer='adam',\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
        "              metrics=['accuracy'])\n",
        "# Start timer\n",
        "start = datetime.datetime.now()\n",
        "history = model12.fit(train_cnn\n",
        "                    ,train_labels_cnn\n",
        "                    ,epochs=200\n",
        "                    ,batch_size=100\n",
        "                    ,validation_data=(val_cnn,val_labels_cnn)\n",
        "                    ,callbacks=[\n",
        "                    tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=3)\n",
        "                    ]                                                                                                           \n",
        "                   )\n",
        "\n",
        "# Record the time it takes\n",
        "duration = datetime.datetime.now() - start\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ww34JHZBIYeK"
      },
      "source": [
        "test_loss, test_accuracy = model12.evaluate(test_cnn, test_labels)\n",
        "print('test set accuracy: ', test_accuracy)\n",
        "preds = model12.predict(test_cnn)\n",
        "print('shape of preds: ', preds.shape)\n",
        "history_dict = history.history\n",
        "history_dict.keys()\n",
        "history_df=pd.DataFrame(history_dict)\n",
        "#history_df.tail().round(3)\n",
        "last_train_acc=history_df.tail(1).accuracy.values\n",
        "last_val_acc=history_df.tail(1).val_accuracy.values\n",
        "last_train_loss=history_df.tail(1).loss.values\n",
        "last_val_loss=history_df.tail(1).val_loss.values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g-cQ9WjlFaoQ"
      },
      "source": [
        "##### Plotting Performance Metrics \n",
        "\n",
        "We use Matplotlib to create 2 plots--displaying the training and validation loss (resp. accuracy) for each (training) epoch side by side."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "weVwlT7RIYeL"
      },
      "source": [
        "losses = history.history['loss']\n",
        "accs = history.history['accuracy']\n",
        "val_losses = history.history['val_loss']\n",
        "val_accs = history.history['val_accuracy']\n",
        "epochs = len(losses)\n",
        "plt.figure(figsize=(16, 4))\n",
        "for i, metrics in enumerate(zip([losses, accs], [val_losses, val_accs], ['Loss', 'Accuracy'])):\n",
        "    plt.subplot(1, 2, i + 1)\n",
        "    plt.plot(range(epochs), metrics[0], label='Training {}'.format(metrics[2]))\n",
        "    plt.plot(range(epochs), metrics[1], label='Validation {}'.format(metrics[2]))\n",
        "    plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kA3uiYVNIYeL"
      },
      "source": [
        "data12=dict()\n",
        "data12['Model Name'] = \"Model12 CNN with 2 convolution/max pooling layers (dropout regularization-0.2)\"\n",
        "data12['train_loss'] = history_dict['loss'][-1]\n",
        "data12['train_acc'] = history_dict['accuracy'][-1]\n",
        "data12['val_loss'] = history_dict['val_loss'][-1]\n",
        "data12['val_acc'] = history_dict['val_accuracy'][-1]\n",
        "data12['Test Accuracy']=test_accuracy\n",
        "data12['Test Loss']=test_loss\n",
        "data12['Time (s)']=duration\n",
        "metrics3.append(data12)\n",
        "metrics3\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bCUCyNUcIYeM"
      },
      "source": [
        "### Experiment 13: CNN with 3 convolution/max pooling layers (dropout regularization- dropout size: 0.2)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zTGwo_CvIYeM"
      },
      "source": [
        "model13 = models.Sequential(name=\"model13\")\n",
        "model13.add(layers.Conv2D(filters=128, kernel_size=(3, 3), strides=(1, 1), activation=tf.nn.relu,input_shape=(32, 32, 3)))\n",
        "model13.add(layers.MaxPool2D((2, 2),strides=2))\n",
        "model13.add(layers.Dropout(0.2))\n",
        "model13.add(layers.Conv2D(filters=256, kernel_size=(3, 3), strides=(1, 1), activation=tf.nn.relu))\n",
        "model13.add(layers.MaxPool2D(pool_size=(2, 2),strides=2))\n",
        "model13.add(layers.Dropout(0.2))\n",
        "model13.add(layers.Conv2D(filters=512, kernel_size=(3, 3), strides=(1, 1), activation=tf.nn.relu))\n",
        "model13.add(layers.MaxPool2D(pool_size=(2, 2),strides=2))\n",
        "model13.add(layers.Dropout(0.2))\n",
        "model13.add(layers.Flatten())\n",
        "model13.add(layers.Dense(units=384, activation=tf.nn.relu))\n",
        "model13.add(layers.Dense(units=10, activation=tf.nn.softmax))\n",
        "model13.summary()\n",
        "model13.compile(optimizer='adam',\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
        "              metrics=['accuracy'])\n",
        "# Start timer\n",
        "start = datetime.datetime.now()\n",
        "history = model13.fit(train_cnn\n",
        "                    ,train_labels_cnn\n",
        "                    ,epochs=200\n",
        "                    ,batch_size=100\n",
        "                    ,validation_data=(val_cnn,val_labels_cnn)\n",
        "                    ,callbacks=[\n",
        "                    tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=3)\n",
        "                    ]                                                                                                           \n",
        "                   )\n",
        "\n",
        "# Record the time it takes\n",
        "duration = datetime.datetime.now() - start\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o9jAjsJsIYeO"
      },
      "source": [
        "test_loss, test_accuracy = model13.evaluate(test_cnn, test_labels)\n",
        "print('test set accuracy: ', test_accuracy)\n",
        "preds = model13.predict(test_cnn)\n",
        "print('shape of preds: ', preds.shape)\n",
        "history_dict = history.history\n",
        "history_dict.keys()\n",
        "history_df=pd.DataFrame(history_dict)\n",
        "#history_df.tail().round(3)\n",
        "last_train_acc=history_df.tail(1).accuracy.values\n",
        "last_val_acc=history_df.tail(1).val_accuracy.values\n",
        "last_train_loss=history_df.tail(1).loss.values\n",
        "last_val_loss=history_df.tail(1).val_loss.values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q1dk3sX1BQS_"
      },
      "source": [
        "##### Plotting Performance Metrics \n",
        "\n",
        "We use Matplotlib to create 2 plots--displaying the training and validation loss (resp. accuracy) for each (training) epoch side by side."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Neqy-RcsIYeO"
      },
      "source": [
        "losses = history.history['loss']\n",
        "accs = history.history['accuracy']\n",
        "val_losses = history.history['val_loss']\n",
        "val_accs = history.history['val_accuracy']\n",
        "epochs = len(losses)\n",
        "plt.figure(figsize=(16, 4))\n",
        "for i, metrics in enumerate(zip([losses, accs], [val_losses, val_accs], ['Loss', 'Accuracy'])):\n",
        "    plt.subplot(1, 2, i + 1)\n",
        "    plt.plot(range(epochs), metrics[0], label='Training {}'.format(metrics[2]))\n",
        "    plt.plot(range(epochs), metrics[1], label='Validation {}'.format(metrics[2]))\n",
        "    plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eGXC9CVrIYeO"
      },
      "source": [
        "data13=dict()\n",
        "data13['Model Name'] = \"Model13 CNN with 3 convolution/max pooling layers (dropout regularization-0.2)\"\n",
        "data13['train_loss'] = history_dict['loss'][-1]\n",
        "data13['train_acc'] = history_dict['accuracy'][-1]\n",
        "data13['val_loss'] = history_dict['val_loss'][-1]\n",
        "data13['val_acc'] = history_dict['val_accuracy'][-1]\n",
        "data13['Test Accuracy']=test_accuracy\n",
        "data13['Test Loss']=test_loss\n",
        "data13['Time (s)']=duration\n",
        "metrics3.append(data13)\n",
        "metrics3\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UWVVrm5cg97R"
      },
      "source": [
        "results=pd.DataFrame(metrics3,columns=['Model Name','Test Accuracy','Test Loss',\"Time (s)\",'train_acc',\"train_loss\",\"val_acc\",\"val_loss\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zWXkGMnmjWzt"
      },
      "source": [
        "results"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "04XBVY0pBQS-",
        "scrolled": false
      },
      "source": [
        "results.set_index('Model Name')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yyl_GZOjqwJ8"
      },
      "source": [
        "results.to_csv('results.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bTRSM1b-rdpt"
      },
      "source": [
        "!cp results.csv \"drive/My Drive/\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1DiTogBkrMj8"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QYapgcBBqoXM"
      },
      "source": [
        "pd.set_option('display.max_colwidth', 255)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y8bj0gb4BQS_"
      },
      "source": [
        "## Evaluate the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fCENqAxPBQS_"
      },
      "source": [
        "In order to ensure that this is not a simple \"memorization\" by the machine, we should evaluate the performance on the test set. This is easy to do, we simply use the `evaluate` method on our model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SnGY6erRBQS_"
      },
      "source": [
        "## Predictions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QnpWli3YE-SO"
      },
      "source": [
        "results"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d0WufsyEBQS_"
      },
      "source": [
        "test_loss, test_accuracy = model1.evaluate(test_images_norm, test_labels)\n",
        "print('test set accuracy: ', test_accuracy)\n",
        "preds = model1.predict(test_images_norm)\n",
        "print('shape of preds: ', preds.shape)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qq82z9YhFkUZ"
      },
      "source": [
        "## Plotting Performance Metrics \n",
        "\n",
        "We use Matplotlib to create 2 plots--displaying the training and validation loss (resp. accuracy) for each (training) epoch side by side."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PX_RmAQgBQS_",
        "scrolled": false
      },
      "source": [
        "history_dict = history.history\n",
        "history_dict.keys()\n",
        "history_df=pd.DataFrame(history_dict)\n",
        "history_df.tail().round(3)\n",
        "\n",
        "losses = history.history['loss']\n",
        "accs = history.history['accuracy']\n",
        "val_losses = history.history['val_loss']\n",
        "val_accs = history.history['val_accuracy']\n",
        "epochs = len(losses)\n",
        "plt.figure(figsize=(16, 4))\n",
        "for i, metrics in enumerate(zip([losses, accs], [val_losses, val_accs], ['Loss', 'Accuracy'])):\n",
        "    plt.subplot(1, 2, i + 1)\n",
        "    plt.plot(range(epochs), metrics[0], label='Training {}'.format(metrics[2]))\n",
        "    plt.plot(range(epochs), metrics[1], label='Validation {}'.format(metrics[2]))\n",
        "    plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eo4quLxCBQTA"
      },
      "source": [
        "## Confusion matrices\n",
        "\n",
        "Let us see what the confusion matrix looks like. Using both `sklearn.metrics`. Then we visualize the confusion matrix and see what that tells us."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GIVWUVh2BQTA"
      },
      "source": [
        "Get the predicted classes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7gog1UjgBQTA"
      },
      "source": [
        "pred_classes = np.argmax(model13.predict(test_cnn), axis=-1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DFGIarKqBQTA"
      },
      "source": [
        "### Visualizing the confusion matrix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e8ocOl5iBQTA"
      },
      "source": [
        "conf_mx = tf.math.confusion_matrix(test_labels, pred_classes)\n",
        "conf_mx"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "acsA3Q8rIR3r"
      },
      "source": [
        "pd.DataFrame(conf_mx).to_csv('conf.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KCpUnuFDIgY5"
      },
      "source": [
        "!cp conf.csv \"drive/My Drive/\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JktnOYxKJzci"
      },
      "source": [
        "con_mat = conf_mx.numpy()\n",
        "classes=[0,1,2,3,4,5,6,7,8,9]\n",
        "con_mat_norm = np.around(con_mat.astype('float') / con_mat.sum(axis=1)[:, np.newaxis], decimals=2)\n",
        "\n",
        "con_mat_df = pd.DataFrame(con_mat_norm,\n",
        "                     index = classes, \n",
        "                     columns = classes)\n",
        "figure = plt.figure(figsize=(8, 8))\n",
        "sns.heatmap(con_mat_df, annot=True,cmap=plt.cm.Blues,)\n",
        "plt.tight_layout()\n",
        "plt.ylabel('Actual Label')\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.title('CIFAR10 Confusion Matrix Heatmap')\n",
        "#plt.savefig(\"confusion_matrix_plot_mnist_1\", tight_layout=True)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s8AA5bHLBQTA"
      },
      "source": [
        "plt.figure(figsize=(16,8))\n",
        "plt.matshow(conf_mx, cmap=plt.cm.Blues,fignum=1)\n",
        "plt.xlabel(\"Predicted Classes\")\n",
        "plt.ylabel(\"Actual Classes\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LzxYqvOVBQTA"
      },
      "source": [
        "preds = model13.predict(test_cnn)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fOIByBY-BQTA"
      },
      "source": [
        "preds.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qgK4yZt4BQTB"
      },
      "source": [
        "### Predictions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yur69upzBQTB"
      },
      "source": [
        "cm = sns.light_palette((260, 75, 60), input=\"husl\", as_cmap=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FoXrnXb8BQTB"
      },
      "source": [
        "df = pd.DataFrame(preds[0:20], columns = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck'])\n",
        "df.style.format(\"{:.2%}\").background_gradient(cmap=cm)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "--jq-EuoaTh8"
      },
      "source": [
        "(_,_), (test_images, test_labels) = tf.keras.datasets.cifar10.load_data()\n",
        "\n",
        "img = test_images[2004]\n",
        "img_tensor = image.img_to_array(img)\n",
        "img_tensor = np.expand_dims(img_tensor, axis=0)\n",
        "\n",
        "class_names = ['airplane'\n",
        ",'automobile'\n",
        ",'bird'\n",
        ",'cat'\n",
        ",'deer'\n",
        ",'dog'\n",
        ",'frog' \n",
        ",'horse'\n",
        ",'ship'\n",
        ",'truck']\n",
        "\n",
        "plt.imshow(img, cmap='viridis')\n",
        "plt.axis('off')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v7HMVYT_aUNs"
      },
      "source": [
        "# Extracts the outputs of the top 8 layers:\n",
        "layer_outputs = [layer.output for layer in model13.layers[:8]]\n",
        "# Creates a model that will return these outputs, given the model input:\n",
        "activation_model = models.Model(inputs=model13.input, outputs=layer_outputs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q9sSr6LpBQTB"
      },
      "source": [
        "activations = activation_model.predict(img_tensor)\n",
        "len(activations)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kdf1Xm4Zc0T2"
      },
      "source": [
        "layer_names = []\n",
        "for layer in model13.layers:\n",
        "    layer_names.append(layer.name)\n",
        "    \n",
        "layer_names"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E_KkKxCuBQTB",
        "scrolled": false
      },
      "source": [
        "# These are the names of the layers, so can have them as part of our plot\n",
        "layer_names = []\n",
        "for layer in model13.layers[:9]:\n",
        "    layer_names.append(layer.name)\n",
        "\n",
        "images_per_row = 16\n",
        "\n",
        "# Now let's display our feature maps\n",
        "for layer_name, layer_activation in zip(layer_names, activations):\n",
        "    # This is the number of features in the feature map\n",
        "    n_features = layer_activation.shape[-1]\n",
        "\n",
        "    # The feature map has shape (1, size, size, n_features)\n",
        "    size = layer_activation.shape[1]\n",
        "\n",
        "    # We will tile the activation channels in this matrix\n",
        "    n_cols = n_features // images_per_row\n",
        "    display_grid = np.zeros((size * n_cols, images_per_row * size))\n",
        "\n",
        "    # We'll tile each filter into this big horizontal grid\n",
        "    for col in range(n_cols):\n",
        "        for row in range(images_per_row):\n",
        "            channel_image = layer_activation[0,\n",
        "                                             :, :,\n",
        "                                             col * images_per_row + row]\n",
        "            # Post-process the feature to make it visually palatable\n",
        "            channel_image -= channel_image.mean()\n",
        "            channel_image /= channel_image.std()\n",
        "            channel_image *= 64\n",
        "            channel_image += 128\n",
        "            channel_image = np.clip(channel_image, 0, 255).astype('uint8')\n",
        "            display_grid[col * size : (col + 1) * size,\n",
        "                         row * size : (row + 1) * size] = channel_image\n",
        "\n",
        "    # Display the grid\n",
        "    scale = 1. / size\n",
        "    plt.figure(figsize=(scale * display_grid.shape[1],\n",
        "                        scale * display_grid.shape[0]))\n",
        "    plt.title(layer_name)\n",
        "    plt.grid(False)\n",
        "    plt.imshow(display_grid, aspect='auto', cmap='viridis')\n",
        "    \n",
        "plt.show();"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "92VFZU3nLxfn"
      },
      "source": [
        "model = Sequential()\n",
        "model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same', input_shape=(32, 32, 3)))\n",
        "model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
        "model.add(MaxPooling2D((2, 2)))\n",
        "model.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
        "model.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
        "model.add(MaxPooling2D((2, 2)))\n",
        "model.add(Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
        "model.add(Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
        "model.add(MaxPooling2D((2, 2)))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(128, activation='relu', kernel_initializer='he_uniform'))\n",
        "model.add(Dense(10, activation='softmax'))\n",
        "# compile model\n",
        "\n",
        "model.compile(optimizer='adam',\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
        "              metrics=['accuracy'])\n",
        "# Start timer\n",
        "start = datetime.datetime.now()\n",
        "history = model.fit(train_cnn\n",
        "                    ,train_labels_cnn\n",
        "                    ,epochs=200\n",
        "                    ,batch_size=100\n",
        "                    ,validation_data=(val_cnn,val_labels_cnn)\n",
        "                    ,callbacks=[\n",
        "                    tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=3)\n",
        "                    ]                                                                                                           \n",
        "                   )\n",
        "\n",
        "# Record the time it takes\n",
        "duration = datetime.datetime.now() - start"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LScwWcH5Z60-"
      },
      "source": [
        "test_loss, test_accuracy = model.evaluate(test_cnn, test_labels)\n",
        "print('test set accuracy: ', test_accuracy)\n",
        "preds = model.predict(test_cnn)\n",
        "print('shape of preds: ', preds.shape)\n",
        "history_dict = history.history\n",
        "history_dict.keys()\n",
        "history_df=pd.DataFrame(history_dict)\n",
        "#history_df.tail().round(3)\n",
        "last_train_acc=history_df.tail(1).accuracy.values\n",
        "last_val_acc=history_df.tail(1).val_accuracy.values\n",
        "last_train_loss=history_df.tail(1).loss.values\n",
        "last_val_loss=history_df.tail(1).val_loss.values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MNsA9bjhfbTW"
      },
      "source": [
        "losses = history.history['loss']\n",
        "accs = history.history['accuracy']\n",
        "val_losses = history.history['val_loss']\n",
        "val_accs = history.history['val_accuracy']\n",
        "epochs = len(losses)\n",
        "plt.figure(figsize=(16, 4))\n",
        "for i, metrics in enumerate(zip([losses, accs], [val_losses, val_accs], ['Loss', 'Accuracy'])):\n",
        "    plt.subplot(1, 2, i + 1)\n",
        "    plt.plot(range(epochs), metrics[0], label='Training {}'.format(metrics[2]))\n",
        "    plt.plot(range(epochs), metrics[1], label='Validation {}'.format(metrics[2]))\n",
        "    plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iUOA-EOkfl9J"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}